---
title: Boostcamp 2days
date: 2022-09-20 10:00:00 +0900
comment: true
categories: [Boostcamp AI Tech 4기]
tags: [1weeks, 2day]
---
# 2일차 학습 정리
---
#### AI Math
- **벡터**
  - 공간에서 한 점
  - L1노름, L2노름 --> 차이점 : 기하학적 성질이 다름
  - 제 2코사인 법칙을 이용해 두 벡터 사이의 각도를 계산
    - $$ \cos\theta = \frac {<x, y>}{||x||_2||y||_2} $$
    - 내적 
      - $$ <x,y> = \sum_{i=1}^{d} x_iy_i $$
      - 정사영의 길이를 벡터y의 길이만큼 조정한 값
      - 두 벡터의 유사도를 측정하는데 사용 가능
- **행렬**
  - 행 벡터를 원소로 가지는 2차원 배열
  - 공간에서 여러 점들
  - 행렬 곱셈
    - i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산
    - 벡터를 다른 차원의 공간으로 보낼 수 있음
    - 패턴을 추출할 수 있고 데이터를 압축 할 수 있음
    - 선형변환(linear transform)
  - 행렬 내적
    - np.inner는 i번째 행백터와 j번째 행벡터 사이의 내적을 계산
    - 수학에서의 내적
      - $$XY^T$$
  - 역행렬
    - np.linalg.inv(A)
    - 행과 열의 숫자가 같고 행렬식(determinant)이 0이 아닌 경우에만 계산가능
    - 역행렬을 계산할 수 없다면 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose) 역행렬을 이용
      - np.linalg.pinv(A)
      - 연립방정식 해 구하기 , 선형회귀분석
 
- **경사하강법**
  - 목적함수를 최소화할 때 사용
  - 그레디언트 벡터 사용
  - 미분이 가능하고 볼록한 함수에 대해서 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장
  - 선형회귀의 경우 수렴이 보장되나 비선형회귀의 경우 수렴이 항상 보장되지 않음
    - 변형된 경사하강법을 사용해야함
      - 확률적 경사하강법(SGD: stochastic gradient descent) : 일부데이터를 사용하여 업데이트

- **비선형 모델 -> 신경망**
  - 분류 문제를 풀때 선형모델과 소프트 맥스 함수를 결합하여 예측
    - 소프트맥스(softmax) : 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
  - 신경망 : 선형모델과 활성함수(activation function)를 합성한 함수
  - 활성함수
    - 비선형 함수로 딥러닝에서 매우 중요한 개념
    - sigmoid, tanh, ReLU
      - ![sigmoid, tanh, ReLU](/img/post/boostcamp_2days_img_1.png)
  - 다층 퍼셉트론
    - 신경망이 여러층 합성된 함수
    - 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능
  - 순전파 (forward propagation) : 순차적인 신경망 계산
  - 역전파 (backpropagation) 알고리즘
    - 각 층에 사용된 파라미터를 순차적으로 학습
    - 학습에 필요한 각 층 파라미터의 그레디언트 벡터는 윗층 부터 역순으로 계산
    - 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiaion)을 사용

#### 확률론
- 딥러닝은 확률론 기반의 기계학습 이론을 바탕
  - 회귀분석 손실함수 -> L2-노름 : 예측오차의 분산을 가장 최소화 하는 방향으로 학습
  - 불류문제 손실함수 -> 교차엔트로피(cross-entropy) : 모델 예측의 불확실성을 최소화하는 방향으로 학습
- **확률변수**
  - 확률 분포에 따라 확률변수가 다르게 모델링
  - 이산확률변수(이산형(discrete))
    - 확률변수가 가질수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링
      - $$ \Rho(X\in A)= \sum_{x\in A} P(X=x) $$
  - 연속확률변수(연속형(continuous))
    - 데이터 공간에 정의된 확률변수의 밀도(density)위에서의 적분을 통해 모델링
      - $$ \Rho(X\in A)= \int_{A} P(x)dx $$
- **확률분포**
  - 데이터의 초상화
  - 결합 분포 P(x, y) : 전체 데이터(x, y) 확률 분포를 모델링
    - 주변확률분포 P(x) : 입력x에 대한 주변 확률 분포
      - y에 대한 정보를 주진 않음
    - 조건부확률분포 : 데이터공간에서 입력x와 출력y 사이의 관계를 모델링
      - 조건부확률은 입력 변수x에 대해 정답이 y일 확률을 의미
- **기대값 (평균) (expectation ; mean)**
  - 데이터를 대표하는 통계량
  - 연속확률분포의 경우엔 적분, 이산확률분포의 경우엔 급수
  - 분산, 첨도, 공분산 등 여러 통계량을 계산 가능 
- **몬테카를로(Monte Carlo) 샘플링**
  - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때 가 대부분
  - 확률 분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법을 사용
  - 독립 추출만 보장된다면 대수의 법칙에 의해 수렴성을 보장

#### 통계학
- **통계적 모델링**
  - 적절한 가정 위에서 확률 분포를 추정
  - 실제로는 근사적으로 확률분포를 추정할 수 밖에 없음
- **모수적(parametric) 방법론**
  - 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법
- **비모수적(nonparametric) 방법론**
  - 특정 확률 분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 방법
  - 기계학습의 많은 방법론이 속함
- **확률분포 가정 예제**
  - 방법 -> 히스토그램을 통해 모양을 관찰
  - 데이터가 2개의 값(0 또는 1)만 가지는 경우 -> 베르누이 분포
  - 데이터가 n개의 이산적인 값을 가지는 경우 -> 카테고리 분포
  - 데이터가 [0, 1] 사이에서 값을 가지는 경우 -> 베타 분포
  - 데이터가 0 이상의 값을 가지는 경우 -> 감마 분포, 로그정규분포 등
  - 데이터가 실수전체에서 값을 가지는 경우 -> 정규분포, 라플라스 분포 등
  - 기계적으로 확률 분포를 가정해서는 안되며, 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙
- **표집분포(sampling distribution)**
  - 통계량의 확률 분포
  - 표본분포와 표집분포는 다르다!
- **중심극한정리(Central Limit Theorem)**
  - 표본평균의 표집분포는 N이 커질수록 정규분포를 따른다.
  - 모집단의 분포가 정규분포를 따르지 않아도 성립
- **최대가능도 추정법 (maximum likelihood estimation, MLE)**
  - 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나 
  $$ \hat\theta_{MLE} = argmax L(\theta; x) = argmax P(x|\theta) $$
  - 가능도 함수
    - 데이터가 주어진 상황에서 theta를 변형시킴에 따라 값이 바뀌는 함수
    - 모수 theta를 따르는 분포가 x를 관찰할 가능성 (확률 x)
  - 데이터 집합 X가 독립적으로 추출되었을 경우 로그 가능도를 최적화
    - $$ L(\theta ; X) = \Pi_{i=1}^{n} P(x_i|\theta) \rArr logL(\theta ; X) = \sum_{i=1}^{n} log P(x_i|\theta) $$
- **딥러닝에 사용되는 최대가능도 추정법**
  - $$ \hat\theta_{MLE} = argmax \frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i,k}log(MLP_{\theta}(x_i)_k) $$
  - 정확하게 이해하지 못하여 따로 다시 학습 필요!
- **확률 분포 상이의 거리**
  - 총변동거리, 바슈타인거리
  - 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)
    - 이산확률변수
      - $$ KL(P||Q) = -\Epsilon_{x \sim P(x)}[logQ(x)] + \Epsilon_{x \sim P(x)}[logP(x)]$$
    - 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 동일


---
# 피어세션
- '피어세션이 피어(peer)씁니다' 활동 PPT 제작
- TMI시간을 가지며 어색한 분위기를 해소
- 멘토링 준비
- [회의록]
  
---
### 참고자료
- 부스트캠프 AI Tech 교육 자료